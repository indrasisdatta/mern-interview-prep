PROJECT: AUTO TRIAGING PLATFORM AT Verizon
______________________________________________

The goal is to quickly understand why an order or customer activity failed. 
Right now logs and data are scattered across many systems which makes debugging slow. 
Backend pulls data from ELK into MongoDB. Frontend shows order flows, different metrics to identify issue. 
AI integration - log summarization using LangChain and HuggingFace, so that users can get a quick summary instead of checking hugs logs. 

End users: Business operations team, support team.

Logs are generated by different Verizon microservices during order creation, checkout flows, and customer interactions.
A log aggregator (Filebeat/Logstash) collects them and pushes them into ELK (Elasticsearch).
 - Backend services send logs to ELK.
 - A scheduled ETL (Node.js/Python) pulls only relevant fields from ELK.
 - Logs are transformed → flattened → stored in MongoDB for fast FE queries.

REQUIREMENT: Given any identifier (Order ID, Cart ID, Session ID, Phone number), we show a consolidated timeline of events across multiple systems with logs, screenshots, and error patterns.

Non-functional requirements:
- Low latency (UI < 1s, API < 300ms)
- High availability
- Ability to handle log bursts
- Data freshness (last 15 min)
- Secure PII handling

React window for list virtualization 
Lazy imports, code splitting
React query solves server state: caching, refetching, stale handling, race conditions, background sync.
Redux is used for client state (filters, UI modes, selected node)
Parallel API calls using useQueries
Screenshot rendering: lazy load images with Intersection Observers, Blob caching, skeleton loading 

PERFORMANCE ISSUE SOLVED: 
 - Rendering large log tables. Solved via Virtualization, backend pagination, filtering on server side.
 - Lazy load heavy modules like graph visualizer, AI summary, screenshot viewer.
 - Debounced search and backend throttling

How to handle API timeout? React Query retry with exponential backoff + fallback UI + user retry
How is exponential backoff implemented? 
    useQuery(
        ["orders"],
        fetchOrders,
        {
            retry: (failureCount, error) => {
                // React Query should NOT retry on auth failures
                if (error.response?.status === 401) return false;
                return failureCount < 3;
            }
            retryDelay: (attempt) => {
                const delay = Math.min(1000 * 2 ** attempt, 30000);
                return delay + Math.random() * 500; // jitter
            }
        }
    );

AUTHENTICATION:
- After login, server sends access token (15 mins) and refresh token (1 day)
- Store access token in redux only and refresh token in HttpOnly cookie
- Axios request interceptor attaches Authorization header access token in every request 
- Axios response interceptor checks if status is 401 then access token has expired 
- It silently sends refresh token to refresh endpoint, gets new access token and reties API 

axios.interceptors.request.use((config) => {
  const token = getAccessToken();
  if (token) config.headers.Authorization = `Bearer ${token}`;
  return config;
});

axios.interceptors.response.use(
  res => res,
  async (error) => {
    if (error.response.status === 401) {
      const newToken = await refreshAccessToken();
      error.config.headers.Authorization = `Bearer ${newToken}`;
      return axios(error.config); // retry request
    }
    return Promise.reject(error);
  }
);

AUTHORIZATION:

<Route 
    path="/admin"
    element={
        <ProtectedRoute allowedRoles={['admin']}>
            <AdminPage />
        </ProtectedRoute> 
    }
/>

export const ProtectedRoute = ({ allowedRoles, children }) => {
    const { user } = useAuth();
    if (!user) return AUTHENTICATION:
- After login, server sends access token (15 mins) and refresh token (1 day)
- Store access token in redux only and refresh token in HttpOnly cookie
- Axios request interceptor attaches Authorization header access token in every request 
- Axios response interceptor checks if status is 401 then access token has expired 
- It silently sends refresh token to refresh endpoint, gets new access token and reties API 

import axios from "axios";

export const api = axios.create({
  baseURL: "/api",
});

api.interceptors.request.use((config) => {
  const token = getAccessToken();
  if (token) config.headers.Authorization = `Bearer ${token}`;
  return config;
});

api.interceptors.response.use(
  res => res,
  async (error) => {
    if (error.response.status === 401) {
      const newToken = await refreshAccessToken();
      error.config.headers.Authorization = `Bearer ${newToken}`;
      return axios(error.config); // retry request
    }
    return Promise.reject(error);
  }
);

AUTHORIZATION:

<Route 
    path="/admin"
    element={
        <ProtectedRoute allowedRoles={['admin']}>
            <AdminPage />
        </ProtectedRoute> 
    }
/>

export const ProtectedRoute = ({ allowedRoles, children }) => {
    const { user } = useAuth();
    if (!user) return <Navigate to="/login" replace />;
    if (!allowedRoles.includes(user.role)) return <AccessDenied /> 

    return children;
};
    if (!allowedRoles.includes(user.role)) return <AccessDenied /> 

    return children;
}
We store the user role inside the JWT payload, not inside cookies.
JWT is signed, so even though the client can read the role, it cannot modify it without breaking the signature.
Refresh token is stored in HttpOnly secure cookies, but role and user info never go into cookies.
The frontend extracts the user's role from the access token and applies RBAC using ProtectedRoutes.

// Create a single axios instance and use it in react query
const { data } = useQuery(['orders'], () => api.get('/orders').then(res => res.data));



TESTING:
How to test React Query? Mock API layer + test query hooks using react query test utilities.

How to test components that fetch data? Mock service workers (MSW)

Refer to http://github.dev/indrasisdatta/react-vite-poc/tree/9dd38121e1f00007ffed8125ec460fa940d79adc/src


CHALLENGE FACED: 
1. Slow API loading time - Changed normal axios call to react query with caching 

ISSUE FACED:
1. React Query retries conflicting with Axios refresh-token retry logic - causing multiple duplicated API calls, occassional user logout loops.
   => Disable react query retry for authenticated requests 
      Added onError, onSuccess listeners in React query 

2. Slow API response causing flickering/wrong data when changing tabs (Race condition).
    Tab A mounted → fetch logs A started
    User switched tab → Tab A unmounted
    Tab B mounted → fetch logs B started
    fetch A resolved late → updates state of unmounted component → stale data appears
    FIX: cancelling inflight API calls during component unmount using AbortController / React Query’s built-in request cancellation.

    const controller = new AbortController();
    const res = await api.get(`/logs/${sessionId}`, {
        signal: controller.signal,
    });
    return () => {
        controller.abort(); // cancel old request
    };

    const { data } = useQuery(
        ["logs", sessionId],
        ({ signal }) => api.get(`/logs/${sessionId}`, { signal }),
        { staleTime: 0 }
    );

3. Old customer data showed due to incorrect react query key 
   Issue: useQuery(["logs"], () => api.get(`/logs?sessionId=${sessionId}`));

    Fix:
    useQuery(["logs", sessionId], () => api.get(`/logs?sessionId=${sessionId}`));
    // invalidate old queries
    useEffect(() => {
        queryClient.invalidateQueries(["logs"]);
        queryClient.removeQueries(["logs"]);
    }, [sessionId]);


src/
 ├── pages/
 │    ├── Triaging/
 │    ├── DependencyMatrix/
 │    └── CustomerIssueAnalyzer/
 │
 ├── components/
 │    ├── common/        ← reusable components (Button, Loader, Modal, Table)
 │    └── charts/        ← optional: reusable chart components
 │
 ├── hooks/
 │    ├── useAuth.ts
 │    ├── useDebounce.ts
 │    └── useQueryParams.ts
 │
 ├── utils/
 │    ├── formatDate.ts
 │    ├── normalizeLogs.ts
 │    └── parseAgreementId.ts
 │
 ├── store/
 │    ├── userSlice.ts
 │    └── filtersSlice.ts
 │
 ├── types/
 │    ├── logs.d.ts
 │    ├── customer.d.ts
 │    └── triage.d.ts
 │
 └── App.tsx

CRACO gave us the flexibility of Webpack customization without ejecting the CRA setup.
We used it to enable ES15 builds, disable heavy source maps, optimize chunking, and reduce build times significantly.
It was a safe, incremental improvement compared to switching to Vite mid-project


AI INTEGRATION:

1. RAG system to allow users to ask questions about Verizon internal policy documents.
 - Policy PDFs/text files are uploaded to AWS S3.
 - Embedding Generation
    LangChain pipeline processes each document.
    Text is chunked (≈500 tokens per chunk).
    Each chunk is converted to vector embeddings (e.g., using HuggingFace/Instructor XL).
 - Vector Database
     All embeddings are stored in FAISS, persisted back into S3.(FAISS index file is versioned and updated whenever new docs are uploaded.)
 - Query Workflow - When user types a query:
    Convert query → embedding
    Perform similarity search on FAISS vectors
    Retrieve top-k relevant chunks
    Pass them to an LLM (e.g., Llama/Ollama/HF) as context
    Return a grounded answer (no hallucination)

2) AI-Powered Root Cause Analyzer (Log-Based Search Assistant)
  Input Processing: User enters a session ID in the UI and makes API call. 
  Natural-Language → Kibana DSL
  - Using LangChain, we built a prompt/template that Parses the user query and Generates a Kibana ES Query DSL (Elasticsearch query)
  Example:
    "Find payment failures for session ABC123" →
    LangChain produces correct ES query filters.

  Real-Time Log Search:
    Backend executes the generated DSL query on ELK and retrieves relevant logs.

LLM Reasoning Layer

Logs are summarized.

LLM extracts: error patterns, failed microservice, potential root cause, recommended next action.

LLM output returned to FE.

Frontend Rendering
summary
log timeline
root cause explanation
confidence score

LLM Security:
1. DOMPurify to sanitize HTML and markdown and allow specific tags only
2. Server - API rate limiting, don't expose secrets