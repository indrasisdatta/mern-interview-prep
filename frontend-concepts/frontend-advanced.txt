Webpack Tree shaking - removing unused code from the prod build. It relies on static structure of ES6 modules.
Tree shaking is not possible for:
- Common JS modules using require and module.exports are dynamic and not statically analyzable by Webpack.
- Dynamic imports import(Data).then(...)
- Importing/exporting entire modules 
  import * as utils from './utils';

Dependency graph: Starts from the entry point and when a module imports another module, a dependency is formed.
Webpack recursively follows this dependencies from the entry point to build the entire graph.
_________________________________________________________________________
npm v/s yarn v/s pnpm:

Ref: https://www.dhiwise.com/post/pnpm-vs-npm-vs-yarn-which-javascript-package-manager

npm (package.json, package.lock.json)
- hosts the world's largest software registry
- installs packages in a nested manner, creating separate node_modules folder for each package 
  (ensuring each package gets the exact version of its dependencies, thus using more disk space)
Note: npm 10+ supports parallel install and caching as well

yarn (package.json, yarn.lock)
- uses parallel installation to speed up the process
- uses flat dependency tree to reduce package duplication
  (it can lead to version conflicts if multiple packages depend on the same package but different versions)
- caches packages locally, allowing for offline installs
- checks for integrity and has enhanced security measures

Note: npm 10 and yarn v1 have almost similar performance 
Yarn v3 (Berry) uses Plug n Play (PnP) instead of node_modules
.pnp.cjs -> for an import statement, it checks package location and package dependencies 
.yarn/cache/ folder stores all dependencies as compressed zip files 

pnpm (package.json, pnpm-lock.yaml)
- fast, disk space efficient package manager
- uses symlinks to save disk space and speed up installation
- global store approach where all packages are stored in a single space on the disk
- when package is installed, pnpm links files from the global store to the node_modules folder
- Efficient duplication handling: if multiple projects on the same machine depend on the same package version, pnpm will store the package only once in the global store and link to each project. (Saves disk space and speeds up installation)

Global cache: ~/.pnpm-store
Project node_modules/.pnpm -> synlinks to pnpm-store

For straightforward projects - npm
For faster install and better security - yarn
For optimal performance and disk space efficiency - pnpm

__________________________________________________________________

1. What is PWA?
   - can be installed on device and work offline
   - all static assets (part of build files) are cached so that page loads faster on subsequent visits
   - updates are downloaded in the background
   manifest.json, service-worker.js

2. What are service workers?
   - Script that runs independently in the browser background
   - commonly used in 'offline first' applications
   - Uses: Background sync, push notifications
   (Learn howbackground sync and push notifications are implemented)
   
3. What are CSR, SSR, CSG?
   Ref: https://res.cloudinary.com/practicaldev/image/fetch/s--bXHuAxci--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8onh7r5sxmss9f87k726.png
   - CSR: initially loads empty HTML file, then JS and CSS are loaded. 
           Users see blank screen initially. Also, it's not SEO friendly.
   - SSR: each page initiates request to app server, dynamically renders and serves HTML. Advantages: better performance, SEO friendly.
   - SSG: instead of rendering pages on every request, generate files in build time so that they are served immediately. Useful for static pages eg. blog, portfolio sites.
   - ISR: Incremental Static Regeneration is the next improvement to SSG. 
     Periodically builds and revalidates new pages so that content never gets too much outdated.
     Use static generation on a per page basis, without needing to rebuild the entire site.  

   - Pre-rendering: used for creating SEO friendly applications with client side rendering. Have 2 verions for the app - one for humans and one for web crawlers.
   Prerender.io, SSR proxy 
   (Learn how to implement SSR in React)

4. How to ensure cross-browser compatibility?
5. Discuss the importance of accessibility in web development
6. Compare and contrast client-side and server-side state management approaches in web development. When would you choose one over the other?
7. How to use automated testing in frontend? End to end testing using Cypress/Selenium?
8. Real time updates - sockets etc.

=======================================================================

WEB VITALS: standardized metrics set by Google (in 2020) that quantify user experience of a website. 

CORE WEB VITALS: 3 aspects of UX (loading, interactivity, visual stability).
1. Largest contentful paint (LCP) - within 2.5 secs
   => Measures by calculating the time taken to load the largest image/text content since the page first started loading 
2. Interaction to next paint (INP) - within 200ms
   => Checks latency of click, tap, keyboard interactions thoughout the lifespan of user's visit to a page.
3. Cumulative layout shift (CLS) - 0.1 or less	
   => CLS measures how much content elements move around as the page loads. 
      Eg. a poor CLS score would result if a user were reading a page and suddenly a banner loaded and the page jumped down.

First contentful paint: < 1.8s 
First input delay: time takes for a webpage to respond to first user interaction eg. button click (< 100ms)

Next.js improves:
LCP (SSR, image optimization)
SEO 	

React + AI stack: https://www.builder.io/blog/react-ai-stack

Useful hooks: https://usehooks.com/
ShadCN reusable component library: https://21st.dev/

2025 Tools 
  Styling - Tailwind + Shadcn UI 
  Client state management - Zustand 
  Server state management - Tanstack Query
  Table - Tanstack table 
  Forms - React hooks form (validation using Yup or Zod)
  Component development - Storybook
  DB - Supabase (Open source PostgreSQL BE with an option of self-hosting without vendor lock-in)

JS Top trends: https://risingstars.js.org/2024/en
JS Tooling - Vite, Biome, Bun

=======================================================================

BROWSER RENDERING PIPELINE
https://developer.mozilla.org/en-US/docs/Web/Performance/Guides/How_browsers_work

Step 1: Parsing (The Building Phase)
   1) HTML Parsing & DOM Construction: The browser reads the HTML code and converts it into a tree structure called the Document Object Model (DOM). This represents the content and structure of the page.

   2) CSS Parsing & CSSOM Construction: The browser parses all CSS (linked, internal, and inline). It creates the CSS Object Model (CSSOM), which is a map of all the styles that need to be applied to the DOM.

   3) The Preload Scanner: While the main thread is busy building the DOM, this "helper" scans the rest of the document to find and start downloading external files like images, scripts, and fonts early.

   4) JavaScript Execution: 
      4.1) AST Generation: Scripts are parsed into an Abstract Syntax Tree (AST), which is a structured map the computer understands. 
      4.2) Compilation: The engine converts this AST into Bytecode so it can run quickly. 
      4.3) Blocking: By default, JavaScript blocks the pipeline. The browser stops building the DOM while JS runs, because JS might change the HTML or CSS.

   5) Accessibility Tree (AOM): The browser creates a "semantic" version of the page for screen readers. This tree is updated whenever the DOM changes so assistive devices always know what is on the screen.

Step 2: Render (The Visual Calculation Phase)
   1) Style (The Render Tree): The browser combines the DOM and CSSOM into a Render Tree. 
      1.1) It starts at the root and checks which nodes are "visible." 
      1.2) It ignores elements with display: none (they aren't in the tree). 
      1.3) It includes elements with visibility: hidden because they still take up space.

   2) Layout (Geometry): The browser calculates the exact size and position of every object on the page. 
      2.1) Layout: This is the first time the browser calculates these positions. 2.2) Reflow: This is any subsequent calculation caused by changes (like resizing the window or adding new HTML elements).

   3) Paint: The browser fills in the pixels. It draws the text, colors, shadows, and images onto different layers.

   4) Compositing: To make things fast, the browser draws different parts of the page on separate layers (like a fixed header or a video). 
      4.1) It sends these layers to the GPU (Graphics Card). 
      4.2) The GPU flattens them together in the right order to show the final frame on your screen.

Step 3: Interactivity (The Response Phase)
   - Main Thread Free: Once the painting is done, the main thread is finally free to handle user inputs like clicks and typing.
   - Execution: If the JavaScript from Step 1 was too heavy, the page might "look" ready but won't respond to clicks yet (this is known as the "Uncanny Valley" of performance).

Changing width:	Layout → Paint → Composite	(High performance cost)
Changing color:	Paint → Composite	(Medium)
Changing transform:	Composite Only	(Fastest)

requestAnimationFrame - schedules work right before browser repaint, making animations smoother than timers. 
Used for - animations, drag & drop, smooth scrolling. 
Synchs with browser rendering pipeline and automatically pauses in background tabs.
setTimeout() => can run out of synch with the paint and keeps running even when tab is changed. 
At least 60 FPS -> 60 frames in 1000 ms, 1 frame in 1000/60 = 16.7ms 
Browser has to render 1 frame in 16ms.
Within 16 ms, browser has to run JS, layout, paint, composite. 
requestAnimationFrame runs once per frame but if JS takes longer time, the frame is dropped. 

Best practices to avoid Reflow, Repaint: 
 - Prefer transform & opacity (GPU-accelerated)
 - Use requestAnimationFrame for animations
 - batch DOM changes (instead of calling appendChild 10 times, use DocumentFragment to build it in memory and append once) 
 - Use content-visibility: auto for long list or hidden section 
   (Skip the rendering and layout work when it's off-screen)
   If an element is below the fold, and has the content-visibility: auto; rule, the browser does not consider any of its contents. Hence, rendering for that element is skipped. 

Layout Thrashing: browser is forced to repeatedly recalculate layout (reflow) due to repeatedly mixing DOM reads and writes 

// Layout thrashing 
refs.forEach(ref => {
   const offset = ref.current.offsetWidth;
   ref.current.style.width = offset + 20 + 'px';
});
// Prevent - batch read then write
const offsets = refs.map(ref => ref.current.offsetWidth); 
refs.forEach((ref, i) => {
   ref.current.style.width = offsets[i] + 20 + 'px';
}

How to prevent JS memory leaks?
 - clearTimeout and clearInterval 
 - remove event listeners 
 - avoig global vars 
 - in closure inner function, don't reference a very large outer array as it can't be garbage collected 
 - use WeakSet, WeakMap for cache or metadata tied to objects

WeakSet/WeakMap - key as object, allows garbage collection, not iterable  
Set/Map - string, number, key as object, GC not allowed, Iterable

const map = new WeakMap();
let obj = {id: 1};
map.set(obj, 'obj val');
obj = {};
// After sometime 
console.log(map); // Returns for Map but {} for WeakMap